# This configuration file for Fluentd / td-agent is used
# to watch changes to Docker log files. The kubelet creates symlinks that
# capture the pod name, namespace, container name & Docker container ID
# to the docker logs for pods in the /var/log/containers directory on the host.
# If running this fluentd configuration in a Docker container, the /var/log
# directory should be mounted in the container.
#
# These logs are then submitted to Elasticsearch which assumes the
# installation of the fluent-plugin-elasticsearch & the
# fluent-plugin-kubernetes_metadata_filter plugins.
# See https://github.com/uken/fluent-plugin-elasticsearch &
# https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter for
# more information about the plugins.
# Maintainer: Jimmi Dyson <jimmidyson@gmail.com>
#
# Example
# =======
# A line in the Docker log file might look like this JSON:
#
# {"log":"2014/09/25 21:15:03 Got request with path wombat\n",
#  "stream":"stderr",
#   "time":"2014-09-25T21:15:03.499185026Z"}
#
# The time_format specification below makes sure we properly
# parse the time format produced by Docker. This will be
# submitted to Elasticsearch and should appear like:
# $ curl 'http://elasticsearch-logging:9200/_search?pretty'
# ...
# {
#      "_index" : "logstash-2014.09.25",
#      "_type" : "fluentd",
#      "_id" : "VBrbor2QTuGpsQyTCdfzqA",
#      "_score" : 1.0,
#      "_source":{"log":"2014/09/25 22:45:50 Got request with path wombat\n",
#                 "stream":"stderr","tag":"docker.container.all",
#                 "@timestamp":"2014-09-25T22:45:50+00:00"}
#    },
# ...
#
# The Kubernetes fluentd plugin is used to write the Kubernetes metadata to the log
# record & add labels to the log record if properly configured. This enables users
# to filter & search logs on any metadata.
# For example a Docker container's logs might be in the directory:
#
#  /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b
#
# and in the file:
#
#  997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log
#
# where 997599971ee6... is the Docker ID of the running container.
# The Kubernetes kubelet makes a symbolic link to this file on the host machine
# in the /var/log/containers directory which includes the pod name and the Kubernetes
# container name:
#
#    synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log 
#    ->
#    /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log
#
# The /var/log directory on the host is mapped to the /var/log directory in the container
# running this instance of Fluentd and we end up collecting the file:
#
#   /var/log/containers/synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
#
# This results in the tag:
#
#  var.log.containers.synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
#
# The Kubernetes fluentd plugin is used to extract the namespace, pod name & container name
# which are added to the log message as a kubernetes field object & the Docker container ID
# is also added under the docker field object.
# The final tag is:
#
#   kubernetes.var.log.containers.synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
#
# And the final log record look like:
#
# {
#   "log":"2014/09/25 21:15:03 Got request with path wombat\n",
#   "stream":"stderr",
#   "time":"2014-09-25T21:15:03.499185026Z",
#   "kubernetes": {
#     "namespace": "default",
#     "pod_name": "synthetic-logger-0.25lps-pod",
#     "container_name": "synth-lgr"
#   },
#   "docker": {
#     "container_id": "997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b"
#   }
# }
#
# This makes it easier for users to search for logs by pod name or by
# the name of the Kubernetes container regardless of how many times the
# Kubernetes pod has been restarted (resulting in a several Docker container IDs).
#
# TODO: Propagate the labels associated with a container along with its logs
# so users can query logs using labels as well as or instead of the pod name
# and container name. This is simply done via configuration of the Kubernetes
# fluentd plugin but requires secrets to be enabled in the fluent pod. This is a
# problem yet to be solved as secrets are not usable in static pods which the fluentd
# pod must be until a per-node controller is available in Kubernetes.

# Do not directly collect fluentd's own logs to avoid infinite loops.
<match fluent.**>
  type null
</match>

#<source>
#  type systemd
#  path /run/log/journal
#  #filters [{ "_SYSTEMD_UNIT": "kube-proxy.service" }]
#  pos_file /var/log/run-log-journald.pos
##  time_format %Y-%m-%dT%H:%M:%S%:z
#  tag journald
#  strip_underscores true
#  read_from_head true
#  keep_time_key true
#</source>
#<match journald>
#  type stdout
#</match>
#<filter journald>
#  type record_transformer
#  <record>
#    time ${record["@timestamp"]}
#  </record>
#</filter>

# Example:
# {"log":"[info:2016-02-16T16:04:05.930-08:00] Some log text here\n","stream":"stdout","time":"2016-02-17T00:04:05.931087621Z"}
<source>
  type tail
  path /var/log/containers/*.log
  pos_file /var/log/es-containers.log.pos
  time_format %Y-%m-%dT%H:%M:%S.%NZ
  tag docker.*
  format json
  read_from_head true
  keep_time_key true
</source>

# Example:
# 2015-12-21 23:17:22,066 [salt.state       ][INFO    ] Completed state [net.ipv4.ip_forward] at time 23:17:22.066081
<source>
  type tail
  format /^(?<time>[^ ]* [^ ,]*)[^\[]*\[[^\]]*\]\[(?<severity>[^ \]]*) *\] (?<message>.*)$/
  time_format %Y-%m-%d %H:%M:%S
  path /var/log/salt/minion
  pos_file /var/log/es-salt.pos
  tag salt
</source>

# Example:
# Dec 21 23:17:22 gke-foo-1-1-4b5cbd14-node-4eoj startupscript: Finished running startup script /var/run/google.startup.script
<source>
  type tail
  format syslog
  path /var/log/startupscript.log
  pos_file /var/log/es-startupscript.log.pos
  tag startupscript
</source>

# Examples:
# time="2016-02-04T06:51:03.053580605Z" level=info msg="GET /containers/json"
# time="2016-02-04T07:53:57.505612354Z" level=error msg="HTTP Error" err="No such image: -f" statusCode=404
<source>
  type tail
  format /^time="(?<time>[^)]*)" level=(?<severity>[^ ]*) msg="(?<message>[^"]*)"( err="(?<error>[^"]*)")?( statusCode=($<status_code>\d+))?/
  time_format %Y-%m-%dT%H:%M:%S.%NZ
  path /var/log/docker.log
  pos_file /var/log/es-docker.log.pos
  tag docker
</source>

# Example:
# 2016/02/04 06:52:38 filePurge: successfully removed file /var/etcd/data/member/wal/00000000000006d0-00000000010a23d1.wal
<source>
  type tail
  # Not parsing this, because it doesn't have anything particularly useful to
  # parse out of it (like severities).
  format none
  path /var/log/etcd.log
  pos_file /var/log/es-etcd.log.pos
  tag etcd
</source>

# Multi-line parsing is required for all the kube logs because very large log
# statements, such as those that include entire object bodies, get split into
# multiple lines by glog.

# Example:
# I0204 07:32:30.020537    3368 server.go:1048] POST /stats/container/: (13.972191ms) 200 [[Go-http-client/1.1] 10.244.1.3:40537]
<source>
  type tail
  format multiline
  format_firstline /^\w\d{4}/
  format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
  time_format %m%d %H:%M:%S.%N
  path /var/log/kubelet.log
  pos_file /var/log/es-kubelet.log.pos
  tag kubelet
</source>

# Example:
# I0204 07:00:19.604280       5 handlers.go:131] GET /api/v1/nodes: (1.624207ms) 200 [[kube-controller-manager/v1.1.3 (linux/amd64) kubernetes/6a81b50] 127.0.0.1:38266]
<source>
  type tail
  format multiline
  format_firstline /^\w\d{4}/
  format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
  time_format %m%d %H:%M:%S.%N
  path /var/log/kube-apiserver.log
  pos_file /var/log/es-kube-apiserver.log.pos
  tag kube-apiserver
</source>

# Example:
# I0204 06:55:31.872680       5 servicecontroller.go:277] LB already exists and doesn't need update for service kube-system/kube-ui
<source>
  type tail
  format multiline
  format_firstline /^\w\d{4}/
  format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
  time_format %m%d %H:%M:%S.%N
  path /var/log/kube-controller-manager.log
  pos_file /var/log/es-kube-controller-manager.log.pos
  tag kube-controller-manager
</source>

# Example:
# W0204 06:49:18.239674       7 reflector.go:245] pkg/scheduler/factory/factory.go:193: watch of *api.Service ended with: 401: The event in requested index is outdated and cleared (the requested history has been cleared [2578313/2577886]) [2579312]
<source>
  type tail
  format multiline
  format_firstline /^\w\d{4}/
  format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
  time_format %m%d %H:%M:%S.%N
  path /var/log/kube-scheduler.log
  pos_file /var/log/es-kube-scheduler.log.pos
  tag kube-scheduler
</source>

################## BEGIN custom conf #########################

<filter docker.**>
  type kubernetes_metadata
</filter>
# Strip "\n" from the end of the docker logs
<match docker.**>
  remove_prefix docker
  add_prefix kubernetes
  type rewrite
  <rule>
    key     log
    pattern $\n
    replace
  </rule>
</match>

################## Ingress/nginx logs parser ##################

<filter kubernetes.var.log.containers.nginx-ingress-**.log>
  type parser
  format /^(?<host>[^ ]*) (?<domain>[^ ]*) \[(?<x_forwarded_for>[^\]]*)\] (?<server_port>[^ ]*) (?<user>[^ ]*) \[(?<time>[^\]]*)\] "(?:(?<method>\S+[^\"])(?: +(?<path>[^\"]*?)(?: +(?<protocol>\S*))?)?)?" (?<code>[^ ]*) (?<size>[^ ]*)(?: "(?<referer>[^\"]*)" "(?<agent>[^\"]*)")? (?<request_length>[^ ]*) (?<request_time>[^ ]*) (?:\[(?<proxy_upstream_name>[^\]]*)\] )?(?<upstream_addr>[^ ]*) (?<upstream_response_length>[^ ]*) (?<upstream_response_time>[^ ]*) (?<upstream_status>[^ ]*)$/
  time_format %d/%b/%Y:%H:%M:%S %z
  key_name log
  types server_port:integer,code:integer,size:integer,request_length:integer,request_time:float,upstream_response_length:integer,upstream_response_time:float,upstream_status:integer
  reserve_data yes
</filter>
<filter kubernetes.var.log.containers.nginx-ingress-**.log>
  type geoip

  # Specify one or more geoip lookup field which has ip address (default: host)
  # in the case of accessing nested value, delimit keys by dot like 'host.ip'.
  geoip_lookup_key  host

  # Specify optional geoip database (using bundled GeoLiteCity databse by default)
  geoip_database    "/opt/GeoLiteCity.dat"

  # Set adding field with placeholder (more than one settings are required.)
  <record>
    city            ${city["host"]}
    lat             ${latitude["host"]}
    lon             ${longitude["host"]}
    country_code3   ${country_code3["host"]}
    country         ${country_code["host"]}
    country_name    ${country_name["host"]}
    dma             ${dma_code["host"]}
    area            ${area_code["host"]}
    region          ${region["host"]}
    geoip           '{"location":[${longitude["host"]},${latitude["host"]}]}'
  </record>

  # To avoid get stacktrace error with `[null, null]` array for elasticsearch.
  skip_adding_null_record  true

  # Set log_level for fluentd-v0.10.43 or earlier (default: warn)
  log_level         info

  # Set buffering time (default: 0s)
  flush_interval    1s
</filter>

################## Process elasticsearch logs ##################

<filter kubernetes.var.log.containers.es-**monitoring_es**.log>
  # Convert multiline logs into oneliners
  type concat
  # Suppress timeout warnings
  log_level fatal
  key log
  multiline_start_regexp /^[^\s]+/
  flush_interval 1s
</filter>

####### Process elasticsearch logs in default namespace ########

# "default" namespace
<filter kubernetes.var.log.containers.es-**default_es**.log>
  # Convert multiline logs into oneliners
  type concat
  # Suppress timeout warnings
  log_level fatal
  key log
  multiline_start_regexp /^[^\s]+/
  flush_interval 1s
</filter>

################## Process java multiline logs ##################

<filter kubernetes.var.log.containers.java-app-**>
  # Convert multiline logs into oneliners
  type concat
  # Suppress timeout warnings
  log_level fatal
  key log
  multiline_start_regexp /^[^\s]+/
  flush_interval 1s
</filter>

<filter kubernetes.var.log.containers.java-app-**>
  # Convert multiline logs with prefix into oneliners
  # [2016-10-25T01:15:35.018][Instance 1][Port 12345] log log log
  # [2016-10-25T01:15:35.018][Instance 1][Port 12345]     multiline
  type concat
  # Suppress timeout warnings
  log_level fatal
  key log
  multiline_start_regexp /^(?:\[[^\]]+\])+\s[^\s]+/
  flush_interval 1s
</filter>

################## Strip fluentd concat logs ##################

<match kubernetes.var.log.containers.fluentd-elasticsearch-**.log>
  type rewrite_tag_filter
  rewriterule1 log "\[warn\]: dump an error event: error_class=Fluent::ConcatFilter::TimeoutError" clear.fluentd.concat
</match>
<match clear.fluentd.concat>
  type null
</match>

################## Process Kibana logs ##################
# remove raw kibana json logs
#<filter kubernetes.var.log.containers.kibana-logging-v2-**.log>
#  type record_transformer
#  remove_keys log
#</filter>

# Strip kibana logs (we already have nginx logs)
<match kubernetes.var.log.containers.kibana-logging-v2-**.log>
  type null
</match>

################## Process primary app logs ##################

<match kubernetes.var.log.containers.app-primary-**.log>
  type color_stripper
  tag  app-primary-stripped-color
  strip_fields log
</match>
<filter app-primary-stripped-color>
  # Parsing multiline cypher errors
  type concat
  # Suppress timeout warnings
  log_level fatal
  key log
  multiline_start_regexp /^Error with query/
  multiline_end_regexp /^Parameters {/
  flush_interval 1s
</filter>
<filter app-primary-stripped-color>
  # Convert multiline logs into oneliners
  type concat
  # Suppress timeout warnings
  log_level fatal
  key log
  multiline_start_regexp /^[^\s]+/
  flush_interval 1s
</filter>
<filter app-primary-stripped-color>
  type parser
  format /^\[(?<method>\S+)\] (?<path>[^ ]*) \((?<code>[^ ]*)\)$/
  time_format %d/%b/%Y:%H:%M:%S %z
  key_name log
  types code:integer
  reserve_data yes
  suppress_parse_error_log true
</filter>

################## Process users app logs ##################

<match kubernetes.var.log.containers.app-users-**.log>
  type color_stripper
  tag  app-users-stripped-color
  strip_fields log
</match>
<filter app-users-stripped-color>
  # Convert multiline query logs into oneliners
  type concat
  # Suppress timeout warnings
  log_level fatal
  key log
  multiline_start_regexp /^Executing/
  multiline_end_regexp /(?:order by id|;$)/
  flush_interval 1s
</filter>
<filter app-users-stripped-color>
  # Convert multiline logs into oneliners
  type concat
  # Suppress timeout warnings
  log_level fatal
  key log
  multiline_start_regexp /^[^\s]+/
  flush_interval 1s
</filter>
<match app-users-stripped-color>
  # Add query prefix
  type rewrite_tag_filter
  rewriterule1 log "^Executing" query.${tag}
</match>
<filter app-users-stripped-color>
  type parser
  # nodejs prints time and size with the units which are actually a string. "upstream_response_time" and "upstream_response_length"
  # were already defined as integer in nginx parser, so we have to use another names
  format /^(?<method>\S+) (?<path>[^ ]*) (?<code>[^ ]*) (?<app_upstream_response_time>[^ ]*) - (?<app_upstream_response_length>[^ ]*)$/
  time_format %d/%b/%Y:%H:%M:%S %z
  key_name log
  types code:integer
  reserve_data yes
  suppress_parse_error_log true
</filter>

################## Process media app logs ##################

<match kubernetes.var.log.containers.app-media-**.log>
  type color_stripper
  tag  app-media-stripped-color
  strip_fields log
</match>
<filter app-media-stripped-color>
  # Convert multiline logs into oneliners
  type concat
  # Suppress timeout warnings
  log_level fatal
  key log
  multiline_start_regexp /^[^\s]+/
  flush_interval 1s
</filter>
<filter app-media-stripped-color>
  type parser
  # GET /media.jpg\n  Accept: image/png, image/svg+xml, image/*;q=0.8, */*;q=0.5\n  Status: 200 OK 0.00003s
  format /^(?<method>\S+) (?<path>[^ ]*)\n+\s+(?:Params: \[(?<params>[^\]]+)\])?(?:\n|.)*Status: (?<code>[^ ]*).*/
  time_format %d/%b/%Y:%H:%M:%S %z
  key_name log
  types code:integer
  reserve_data yes
  suppress_parse_error_log true
</filter>
<match app-media-stripped-color>
  # add "/documents" prefix to make this path searchable along with nginx logs
  type rewrite
  add_prefix    prefixed
  <rule>
    key     path
    pattern (/.+)$
    replace /documents\1
  </rule>
</match>
# Remove "DELETED" Kubernetes events
#<match kubernetes.var.log.containers.kubernetes-events-printer-**>
#  type rewrite_tag_filter
#  rewriterule1 type ^DELETED$ clear.deleted.kubernetes-events
#</match>
#<match clear.deleted.kubernetes-events>
#  type null
#</match>
# Rewrite "log" with "object.message"
<filter kubernetes.var.log.containers.kubernetes-events-printer-**>
  type record_transformer
  enable_ruby
  <record>
    log ${record['object']['message']}
  </record>
</filter>
################## END custom conf #########################

# Example:
# I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf
<source>
  type tail
  format multiline
  multiline_flush_interval 5s
  format_firstline /^\w\d{4}/
  format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
  time_format %m%d %H:%M:%S.%N
  path /var/log/glbc.log
  pos_file /var/log/es-glbc.log.pos
  tag glbc
</source>

# Example:
# I0603 15:31:05.793605       6 cluster_manager.go:230] Reading config from path /etc/gce.conf
<source>
  type tail
  format multiline
  multiline_flush_interval 5s
  format_firstline /^\w\d{4}/
  format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
  time_format %m%d %H:%M:%S.%N
  path /var/log/cluster-autoscaler.log
  pos_file /var/log/es-cluster-autoscaler.log.pos
  tag cluster-autoscaler
</source>

<match **>
   type elasticsearch
   log_level info
   include_tag_key true
   host elasticsearch-logging
   port 9200
   logstash_format true
   # Set the chunk limit the same as for fluentd-gcp.
   buffer_chunk_limit 2M
   # Cap buffer memory usage to 2MiB/chunk * 32 chunks = 64 MiB
   buffer_queue_limit 200
   flush_interval 5s
   # Never wait longer than 5 minutes between retries.
   max_retry_wait 30
   # Disable the limit on the number of retries (retry forever).
   disable_retry_limit
   time_key time
   # Use multiple threads for processing.
   num_threads 8
   # Specify indices template
   template_name logstash
   template_file /etc/elasticsearch-template-es2x.json
</match>
